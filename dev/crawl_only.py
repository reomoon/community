#!/usr/bin/env python3
"""
GitHub Actionsìš© í¬ë¡¤ë§ ì „ìš© ìŠ¤í¬ë¦½íŠ¸
"""

import json
import os
import sys
from datetime import datetime

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.crawlers.site_crawlers import (
    PpomppuCrawler, 
    BobaeCrawler, 
    DcinsideCrawler, 
    FmkoreaCrawler
)

def crawl_all_sites():
    """ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰"""
    print(f"[{datetime.now()}] ğŸ“± ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ë§ ì‹œì‘...")
    
    # Flask ì•± ì´ˆê¸°í™” (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ìš©)
    try:
        from app import create_app
        from app.models import Post, db
        app = create_app()
        print("Flask ì•± ì´ˆê¸°í™” ì„±ê³µ - ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë©ë‹ˆë‹¤")
        use_database = True
    except Exception as e:
        print(f"Flask ì•± ì´ˆê¸°í™” ì‹¤íŒ¨: {e} - JSON íŒŒì¼ë¡œë§Œ ì €ì¥ë©ë‹ˆë‹¤")
        use_database = False
    
    # HTTP í¬ë¡¤ëŸ¬ë“¤ ì´ˆê¸°í™” (ì•ˆì •ì ì¸ í¬ë¡¤ë§)
    crawlers = {
        'bobae': BobaeCrawler(),
        'dcinside': DcinsideCrawler(),
        'ppomppu': PpomppuCrawler(),
        'fmkorea': FmkoreaCrawler()
    }
    
    all_posts = []
    
    for site_name, crawler in crawlers.items():
        try:
            print(f"\n=== {site_name.upper()} í¬ë¡¤ë§ ì‹œì‘ ===")
            posts = crawler.crawl_popular_posts()
            
            if posts:
                # ì‚¬ì´íŠ¸ë³„ ê²°ê³¼ ì €ì¥
                for post in posts:
                    post['crawled_time'] = datetime.now().isoformat()
                    all_posts.append(post)
                
                print(f"âœ… {site_name}: {len(posts)}ê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")
            else:
                print(f"âš ï¸ {site_name}: í¬ë¡¤ë§ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤")
            
        except Exception as e:
            print(f"âŒ {site_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
            
            # í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ íŠ¹ë³„ ì²˜ë¦¬
            if site_name == 'fmkorea':
                print("ì—í¨ì½”ë¦¬ì•„ í¬ë¡¤ë§ ì‹¤íŒ¨ - ë´‡ ì°¨ë‹¨ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ê°€ëŠ¥ì„±")
            
            continue  # ë‹¤ìŒ ì‚¬ì´íŠ¸ ê³„ì† ì§„í–‰
    
    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    if use_database and all_posts:
        try:
            with app.app_context():
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìµœì‹  ë°ì´í„°ë§Œ ìœ ì§€)
                Post.query.delete()
                
                # ìƒˆ ë°ì´í„° ì €ì¥
                for post_data in all_posts:
                    post = Post(
                        title=post_data['title'],
                        url=post_data['url'],
                        author=post_data.get('author'),
                        site=post_data['site'],
                        category=post_data.get('category', 'humor'),
                        views=post_data.get('views'),
                        likes=post_data.get('likes'),
                        comments=post_data.get('comments'),
                        crawled_at=datetime.now()
                    )
                    db.session.add(post)
                
                db.session.commit()
                print(f"ë°ì´í„°ë² ì´ìŠ¤ì— {len(all_posts)}ê°œ ê²Œì‹œë¬¼ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            if 'app' in locals():
                with app.app_context():
                    db.session.rollback()
    
    # output í´ë” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°)
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = os.path.join(output_dir, f"crawl_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'crawl_time': datetime.now().isoformat(),
            'total_posts': len(all_posts),
            'posts': all_posts
        }, f, ensure_ascii=False, indent=2)
    
    print(f"í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œë¬¼")
    print(f"ê²°ê³¼ ì €ì¥: {output_file}")
    
    return all_posts

if __name__ == "__main__":
    crawl_all_sites()