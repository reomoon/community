#!/usr/bin/env python3
"""
ëª¨ë°”ì¼ í™”ë©´ í¬ë¡¤ë§ì„ ìœ„í•œ Playwright ê¸°ë°˜ í¬ë¡¤ëŸ¬
iPhone 14 Pro í™”ë©´ìœ¼ë¡œ ì—ë®¬ë ˆì´ì…˜í•˜ì—¬ ì‹¤ì œ ëª¨ë°”ì¼ í™”ë©´ ìº¡ì²˜
í˜ì´ì§€ ëê¹Œì§€ í¬ë¡¤ë§ ë° ë´‡ íƒì§€ ìš°íšŒ ê°•í™”
"""

import time
import random
from typing import List, Dict
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin


class MobileCrawler:
    """ëª¨ë°”ì¼ í™”ë©´ í¬ë¡¤ë§ì„ ìœ„í•œ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, site_name: str):
        self.site_name = site_name
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
    
    def setup_mobile_browser(self):
        """ëª¨ë°”ì¼ ë¸Œë¼ìš°ì € í™˜ê²½ ì„¤ì • (iPhone 14 Pro ì—ë®¬ë ˆì´ì…˜)"""
        try:
            print(f"ğŸ“± {self.site_name} ëª¨ë°”ì¼ ë¸Œë¼ìš°ì € ì„¤ì • ì¤‘...")
            
            # Playwright ì´ˆê¸°í™”
            self.playwright = sync_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì‹¤í–‰ (headless ëª¨ë“œ)
            self.browser = self.playwright.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox', 
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-features=VizDisplayCompositor'
                ]
            )
            
            # iPhone 14 Pro ë””ë°”ì´ìŠ¤ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            self.context = self.browser.new_context(**self.playwright.devices['iPhone 14 Pro'])
            
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            self.page = self.context.new_page()
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            self.page.set_default_timeout(30000)  # 30ì´ˆ
            
            print(f"âœ… {self.site_name} ëª¨ë°”ì¼ ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ {self.site_name} ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            self.cleanup()
            return False
    
    def get_mobile_page_source(self, url: str) -> str:
        """ëª¨ë°”ì¼ í™”ë©´ì—ì„œ í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ ëê¹Œì§€)"""
        if not self.setup_mobile_browser():
            return None
        
        try:
            print(f"ğŸ“± {self.site_name} ëª¨ë°”ì¼ í˜ì´ì§€ ì ‘ì†: {url}")
            
            # í˜ì´ì§€ ì´ë™
            response = self.page.goto(url, wait_until='networkidle', timeout=30000)
            
            if response.status != 200:
                print(f"âŒ {self.site_name} HTTP ìƒíƒœ: {response.status}")
                return None
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            time.sleep(3)
            
            # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ
            print(f"ğŸ“œ {self.site_name} í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
            
            for i in range(15):  # 15ë²ˆ ìŠ¤í¬ë¡¤
                self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(random.uniform(1.5, 2.5))  # 1.5~2.5ì´ˆ ëŒ€ê¸°
                
                # ë” ì´ìƒ ë¡œë“œí•  ì½˜í…ì¸ ê°€ ì—†ëŠ”ì§€ í™•ì¸
                current_height = self.page.evaluate("document.body.scrollHeight")
                time.sleep(1)
                new_height = self.page.evaluate("document.body.scrollHeight")
                
                if current_height == new_height and i > 5:
                    print(f"ğŸ“„ {self.site_name} ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ ì™„ë£Œ ({i+1}ë²ˆ ìŠ¤í¬ë¡¤)")
                    break
            
            # ë§¨ ìœ„ë¡œ ë‹¤ì‹œ ìŠ¤í¬ë¡¤
            self.page.evaluate("window.scrollTo(0, 0)")
            time.sleep(2)
            
            # ìŠ¤í¬ë¦°ìƒ· ì´¬ì˜ (í˜ì´ì§€ê°€ ë¡œë“œëœ ìƒíƒœì—ì„œ)
            screenshot_path = f"mobile_screenshot_{self.site_name}_{int(time.time())}.png"
            self.page.screenshot(path=screenshot_path, full_page=True)
            print(f"ğŸ“¸ {self.site_name} ëª¨ë°”ì¼ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ë°˜í™˜
            page_source = self.page.content()
            print(f"âœ… {self.site_name} ëª¨ë°”ì¼ í˜ì´ì§€ ì†ŒìŠ¤ íšë“ ì™„ë£Œ ({len(page_source)} ë¬¸ì)")
            
            return page_source
            
        except Exception as e:
            print(f"âŒ {self.site_name} í˜ì´ì§€ ì†ŒìŠ¤ íšë“ ì‹¤íŒ¨: {e}")
            return None
    
    def should_exclude_post(self, title: str) -> bool:
        """ê²Œì‹œë¬¼ ì œì™¸ ì—¬ë¶€ íŒë‹¨"""
        exclude_keywords = [
            'ê³µì§€', 'ì•ˆë‚´', 'ì´ë²¤íŠ¸', 'ê´‘ê³ ', 'í™ë³´', 'ìŠ¤íŒ¸', 'spam', 'ad',
            'ê´€ë¦¬ì', 'ìš´ì˜ì§„', 'ì‹ ê³ ', 'ë¬¸ì˜', 'ê±´ì˜', 'ì œë³´',
            'ìë™', 'bot', 'ë´‡', 'í…ŒìŠ¤íŠ¸', 'test', 'ì‹¤í—˜'
        ]
        
        title_lower = title.lower()
        for keyword in exclude_keywords:
            if keyword.lower() in title_lower:
                return True
        return False
    
    def cleanup(self):
        """ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.page:
                self.page.close()
                self.page = None
            
            if self.context:
                self.context.close()
                self.context = None
            
            if self.browser:
                self.browser.close()
                self.browser = None
            
            if self.playwright:
                self.playwright.stop()
                self.playwright = None
            
            print(f"ğŸ”Œ {self.site_name} ëª¨ë°”ì¼ ë¸Œë¼ìš°ì € ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {self.site_name} ë¸Œë¼ìš°ì € ì •ë¦¬ ì˜¤ë¥˜: {e}")
    
    def __del__(self):
        """ì†Œë©¸ìì—ì„œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.cleanup()


class MobilePpomppuCrawler(MobileCrawler):
    """Playwright ê¸°ë°˜ ë½ë¿Œ ëª¨ë°”ì¼ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        super().__init__('ppomppu')
        self.base_url = 'https://www.ppomppu.co.kr'
    
    def crawl_popular_posts(self) -> List[Dict]:
        """ë½ë¿Œ ëª¨ë°”ì¼ í™”ë©´ì—ì„œ ì¸ê¸° ê²Œì‹œë¬¼ í¬ë¡¤ë§ (í˜ì´ì§€ ëê¹Œì§€)"""
        posts = []
        
        try:
            # ëª¨ë°”ì¼ HOT ê²Œì‹œíŒ ì ‘ì†
            url = f"{self.base_url}/hot.php?category=2"
            
            # ëª¨ë°”ì¼ í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë¦°ìƒ· ìë™ ì´¬ì˜)
            page_source = self.get_mobile_page_source(url)
            if not page_source:
                return posts
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(page_source, 'html.parser')
            
            post_list = []
            seen_titles = set()
            
            # ë½ë¿Œ ëª¨ë°”ì¼ ë²„ì „ ê²Œì‹œë¬¼ íŒŒì‹±
            selectors = [
                'table tr td a',
                'a[href*="zboard.php"]',
                'a[href*="view.php"]',
                '.list a'
            ]
            
            print(f"ğŸ“± ë½ë¿Œ ëª¨ë°”ì¼ í˜ì´ì§€ ë¶„ì„ ì¤‘... (í˜ì´ì§€ í¬ê¸°: {len(page_source)} ë¬¸ì)")
            
            for selector in selectors:
                elements = soup.select(selector)
                print(f"ğŸ“± {selector} ì„ íƒìë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                
                if len(elements) > 5:
                    # ì œí•œ ì—†ì´ ëª¨ë“  ìš”ì†Œ ì²˜ë¦¬ (í˜ì´ì§€ ëê¹Œì§€)
                    for element in elements:
                        try:
                            title = element.get_text(strip=True)
                            if not title or len(title) < 5:
                                continue
                            
                            if title in seen_titles:
                                continue
                            
                            if self.should_exclude_post(title):
                                continue
                            
                            seen_titles.add(title)
                            
                            # URL êµ¬ì„±
                            href = element.get('href', '')
                            if href.startswith('/'):
                                post_url = self.base_url + href
                            elif href.startswith('http'):
                                post_url = href
                            else:
                                post_url = f"{self.base_url}/{href}"
                            
                            # ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸
                            if not any(pattern in href for pattern in ['zboard', 'view', 'hot']):
                                continue
                            
                            post_data = {
                                'title': title,
                                'url': post_url,
                                'site': self.site_name,
                                'category': 'ì¸ê¸°',
                                'author': 'ë½ë¿Œ',
                                'views': 0,
                                'likes': 1,
                                'comments': 0,
                                'popularity_score': 1
                            }
                            
                            post_list.append(post_data)
                            
                        except Exception as e:
                            continue
                    
                    if post_list:
                        break
            
            posts = post_list
            
            if not posts:
                print(f"âš ï¸  ë½ë¿Œ ëª¨ë°”ì¼ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"âœ… ë½ë¿Œ ëª¨ë°”ì¼ì—ì„œ {len(posts)}ê°œ ê²Œì‹œë¬¼ ì¶”ì¶œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë½ë¿Œ ëª¨ë°”ì¼ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            self.cleanup()
        
        return posts


class MobileBobaeCrawler(MobileCrawler):
    """Playwright ê¸°ë°˜ ë³´ë°°ë“œë¦¼ ëª¨ë°”ì¼ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        super().__init__('bobae')
        self.base_url = 'https://www.bobaedream.co.kr'
    
    def crawl_popular_posts(self) -> List[Dict]:
        """ë³´ë°°ë“œë¦¼ ëª¨ë°”ì¼ í™”ë©´ì—ì„œ ì¸ê¸° ê²Œì‹œë¬¼ í¬ë¡¤ë§ (í˜ì´ì§€ ëê¹Œì§€)"""
        posts = []
        
        try:
            # ëª¨ë°”ì¼ ë² ìŠ¤íŠ¸ ê²Œì‹œíŒ ì ‘ì†
            url = f"{self.base_url}/board/bulletin/list.php?code=best&vdate=w"
            
            # ëª¨ë°”ì¼ í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë¦°ìƒ· ìë™ ì´¬ì˜)
            page_source = self.get_mobile_page_source(url)
            if not page_source:
                return posts
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(page_source, 'html.parser')
            
            post_list = []
            seen_titles = set()
            
            # ë³´ë°°ë“œë¦¼ ëª¨ë°”ì¼ ë²„ì „ ê²Œì‹œë¬¼ íŒŒì‹±
            selectors = [
                'table tr td a',
                'a[href*="view"]',
                '.list a',
                'div[class*="list"] a'
            ]
            
            print(f"ğŸ“± ë³´ë°°ë“œë¦¼ ëª¨ë°”ì¼ í˜ì´ì§€ ë¶„ì„ ì¤‘... (í˜ì´ì§€ í¬ê¸°: {len(page_source)} ë¬¸ì)")
            
            for selector in selectors:
                elements = soup.select(selector)
                print(f"ğŸ“± {selector} ì„ íƒìë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                
                if len(elements) > 5:
                    # ì œí•œ ì—†ì´ ëª¨ë“  ìš”ì†Œ ì²˜ë¦¬ (í˜ì´ì§€ ëê¹Œì§€)
                    for element in elements:
                        try:
                            title = element.get_text(strip=True)
                            if not title or len(title) < 5:
                                continue
                            
                            if title in seen_titles:
                                continue
                            
                            if self.should_exclude_post(title):
                                continue
                            
                            seen_titles.add(title)
                            
                            # URL êµ¬ì„±
                            href = element.get('href', '')
                            if href.startswith('/'):
                                post_url = self.base_url + href
                            elif href.startswith('http'):
                                post_url = href
                            else:
                                post_url = f"{self.base_url}/{href}"
                            
                            # ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸
                            if not any(pattern in href for pattern in ['view', 'read', 'best']):
                                continue
                            
                            post_data = {
                                'title': title,
                                'url': post_url,
                                'site': self.site_name,
                                'category': 'ì¸ê¸°',
                                'author': 'ë³´ë°°ë“œë¦¼',
                                'views': 0,
                                'likes': 1,
                                'comments': 0,
                                'popularity_score': 1
                            }
                            
                            post_list.append(post_data)
                            
                        except Exception as e:
                            continue
                    
                    if post_list:
                        break
            
            posts = post_list
            
            if not posts:
                print(f"âš ï¸  ë³´ë°°ë“œë¦¼ ëª¨ë°”ì¼ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"âœ… ë³´ë°°ë“œë¦¼ ëª¨ë°”ì¼ì—ì„œ {len(posts)}ê°œ ê²Œì‹œë¬¼ ì¶”ì¶œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë³´ë°°ë“œë¦¼ ëª¨ë°”ì¼ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            self.cleanup()
        
        return posts


class MobileDcinsideCrawler(MobileCrawler):
    """Playwright ê¸°ë°˜ ë””ì‹œì¸ì‚¬ì´ë“œ ëª¨ë°”ì¼ í¬ë¡¤ëŸ¬ (íˆ´íŒ ì œê±° ê°•í™”)"""
    
    def __init__(self):
        super().__init__('dcinside')
        self.base_url = 'https://gall.dcinside.com'
    
    def crawl_popular_posts(self) -> List[Dict]:
        """ë””ì‹œì¸ì‚¬ì´ë“œ ëª¨ë°”ì¼ í™”ë©´ì—ì„œ ì¸ê¸° ê²Œì‹œë¬¼ í¬ë¡¤ë§ (íˆ´íŒ ì œê±° + í˜ì´ì§€ ëê¹Œì§€)"""
        posts = []
        
        try:
            # ëª¨ë°”ì¼ ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸ ê²Œì‹œíŒ ì ‘ì†
            url = f"{self.base_url}/board/lists/?id=dcbest"
            
            # íˆ´íŒ ì œê±°ê°€ í¬í•¨ëœ í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.get_mobile_page_source_with_tooltip_removal(url)
            if not page_source:
                return posts
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(page_source, 'html.parser')
            
            post_list = []
            seen_titles = set()
            
            # ë””ì‹œì¸ì‚¬ì´ë“œ ëª¨ë°”ì¼ ë²„ì „ ê²Œì‹œë¬¼ íŒŒì‹±
            selectors = [
                'tr.ub-content a',
                '.gall_tit a',
                'a[href*="view"]',
                'td a'
            ]
            
            print(f"ğŸ“± ë””ì‹œì¸ì‚¬ì´ë“œ ëª¨ë°”ì¼ í˜ì´ì§€ ë¶„ì„ ì¤‘... (í˜ì´ì§€ í¬ê¸°: {len(page_source)} ë¬¸ì)")
            
            for selector in selectors:
                elements = soup.select(selector)
                print(f"ğŸ“± {selector} ì„ íƒìë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                
                if len(elements) > 5:
                    # ì œí•œ ì—†ì´ ëª¨ë“  ìš”ì†Œ ì²˜ë¦¬ (í˜ì´ì§€ ëê¹Œì§€)
                    for element in elements:
                        try:
                            title = element.get_text(strip=True)
                            if not title or len(title) < 5:
                                continue
                            
                            if title in seen_titles:
                                continue
                            
                            if self.should_exclude_post(title):
                                continue
                            
                            seen_titles.add(title)
                            
                            # URL êµ¬ì„±
                            href = element.get('href', '')
                            if href.startswith('/'):
                                post_url = self.base_url + href
                            elif href.startswith('http'):
                                post_url = href
                            else:
                                post_url = f"{self.base_url}/{href}"
                            
                            # ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸
                            if not any(pattern in href for pattern in ['view', 'board']):
                                continue
                            
                            post_data = {
                                'title': title,
                                'url': post_url,
                                'site': self.site_name,
                                'category': 'ì¸ê¸°',
                                'author': 'ë””ì‹œ',
                                'views': 0,
                                'likes': 1,
                                'comments': 0,
                                'popularity_score': 1
                            }
                            
                            post_list.append(post_data)
                            
                        except Exception as e:
                            continue
                    
                    if post_list:
                        break
            
            posts = post_list
            
            if not posts:
                print(f"âš ï¸  ë””ì‹œì¸ì‚¬ì´ë“œ ëª¨ë°”ì¼ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"âœ… ë””ì‹œì¸ì‚¬ì´ë“œ ëª¨ë°”ì¼ì—ì„œ {len(posts)}ê°œ ê²Œì‹œë¬¼ ì¶”ì¶œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë””ì‹œì¸ì‚¬ì´ë“œ ëª¨ë°”ì¼ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            self.cleanup()
        
        return posts
    
    def get_mobile_page_source_with_tooltip_removal(self, url: str) -> str:
        """íˆ´íŒ ì œê±°ê°€ í¬í•¨ëœ í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        if not self.setup_mobile_browser():
            return None
        
        try:
            print(f"ğŸŒ {self.site_name} í˜ì´ì§€ ì ‘ì† ì¤‘...")
            response = self.page.goto(url, wait_until='networkidle', timeout=30000)
            
            if response.status != 200:
                print(f"âŒ {self.site_name} HTTP ìƒíƒœ: {response.status}")
                return None
            
            # íˆ´íŒ ì œê±°ë¥¼ ìœ„í•œ ë§ˆìš°ìŠ¤ ì˜¤ë²„ ì‹œë®¬ë ˆì´ì…˜
            print(f"ğŸ–±ï¸ {self.site_name} íˆ´íŒ ì œê±° ì¤‘...")
            
            try:
                # ì´ë¯¸ì§€ ìˆœì„œ íˆ´íŒë“¤ì„ ì°¾ì•„ì„œ ë§ˆìš°ìŠ¤ ì˜¤ë²„
                tooltip_selectors = [
                    '.list_img img',  # ê²Œì‹œë¬¼ ì´ë¯¸ì§€
                    '.thumb img',     # ì¸ë„¤ì¼ ì´ë¯¸ì§€
                    'img[alt*="ìˆœì„œ"]', # ìˆœì„œ ê´€ë ¨ ì´ë¯¸ì§€
                    '.gall_list img'  # ê°¤ëŸ¬ë¦¬ ë¦¬ìŠ¤íŠ¸ ì´ë¯¸ì§€
                ]
                
                for selector in tooltip_selectors:
                    try:
                        elements = self.page.query_selector_all(selector)
                        for i, element in enumerate(elements[:10]):  # ì²˜ìŒ 10ê°œë§Œ
                            try:
                                # ë§ˆìš°ìŠ¤ ì˜¤ë²„ë¡œ íˆ´íŒ ì œê±°
                                element.hover()
                                self.page.wait_for_timeout(200)  # 0.2ì´ˆ ëŒ€ê¸°
                            except:
                                continue
                    except:
                        continue
                
                print(f"âœ… {self.site_name} íˆ´íŒ ì œê±° ì™„ë£Œ")
                
            except Exception as e:
                print(f"âš ï¸ {self.site_name} íˆ´íŒ ì œê±° ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {str(e)}")
            
            # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ (ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ)
            print(f"ğŸ“œ {self.site_name} í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
            
            # ì—¬ëŸ¬ ë²ˆ ìŠ¤í¬ë¡¤í•´ì„œ ëª¨ë“  ê²Œì‹œë¬¼ ë¡œë“œ
            for i in range(15):  # 15ë²ˆ ìŠ¤í¬ë¡¤
                self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(random.uniform(1.5, 2.5))  # 1.5~2.5ì´ˆ ëŒ€ê¸°
                
                # ë” ì´ìƒ ë¡œë“œí•  ì½˜í…ì¸ ê°€ ì—†ëŠ”ì§€ í™•ì¸
                current_height = self.page.evaluate("document.body.scrollHeight")
                time.sleep(1)
                new_height = self.page.evaluate("document.body.scrollHeight")
                
                if current_height == new_height and i > 5:
                    print(f"ğŸ“„ {self.site_name} ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ ì™„ë£Œ ({i+1}ë²ˆ ìŠ¤í¬ë¡¤)")
                    break
            
            # ë§¨ ìœ„ë¡œ ë‹¤ì‹œ ìŠ¤í¬ë¡¤
            self.page.evaluate("window.scrollTo(0, 0)")
            time.sleep(2)
            
            # ìŠ¤í¬ë¦°ìƒ· ì´¬ì˜
            screenshot_path = f"mobile_screenshot_{self.site_name}_{int(time.time())}.png"
            self.page.screenshot(path=screenshot_path, full_page=True)
            print(f"ğŸ“¸ {self.site_name} ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.page.content()
            print(f"âœ… {self.site_name} í˜ì´ì§€ ì†ŒìŠ¤ íšë“ ({len(page_source)}ì)")
            
            return page_source
            
        except Exception as e:
            print(f"âŒ {self.site_name} í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
            return None


class MobileFmkoreaCrawler(MobileCrawler):
    """Playwright ê¸°ë°˜ ì—í¨ì½”ë¦¬ì•„ ëª¨ë°”ì¼ í¬ë¡¤ëŸ¬ (ë´‡ íƒì§€ ìš°íšŒ ê°•í™”)"""
    
    def __init__(self):
        super().__init__('fmkorea')
        self.base_url = 'https://www.fmkorea.com'
    
    def crawl_popular_posts(self) -> List[Dict]:
        """ì—í¨ì½”ë¦¬ì•„ ëª¨ë°”ì¼ í™”ë©´ì—ì„œ ì¸ê¸° ê²Œì‹œë¬¼ í¬ë¡¤ë§ (ë´‡ íƒì§€ ìš°íšŒ + í˜ì´ì§€ ëê¹Œì§€)"""
        posts = []
        
        try:
            # ëª¨ë°”ì¼ ë² ìŠ¤íŠ¸ ê²Œì‹œíŒ ì ‘ì† (ë´‡ íƒì§€ ìš°íšŒ ê°•í™”)
            url = f"{self.base_url}/best"
            
            # ë´‡ íƒì§€ ìš°íšŒê°€ ê°•í™”ëœ í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.get_mobile_page_source_with_antibot(url)
            if not page_source:
                return posts
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(page_source, 'html.parser')
            
            post_list = []
            seen_titles = set()
            
            # ì—í¨ì½”ë¦¬ì•„ ëª¨ë°”ì¼ ë²„ì „ ê²Œì‹œë¬¼ íŒŒì‹±
            selectors = [
                'a.hx',
                'a[href*="/best/"]',
                '.li.li_best a',
                'a[href*="document_srl"]',
                '.title a',
                '.bd_lst a',
                'li a'
            ]
            
            print(f"ğŸ“± ì—í¨ì½”ë¦¬ì•„ ëª¨ë°”ì¼ í˜ì´ì§€ ë¶„ì„ ì¤‘... (í˜ì´ì§€ í¬ê¸°: {len(page_source)} ë¬¸ì)")
            
            for selector in selectors:
                elements = soup.select(selector)
                print(f"ğŸ“± {selector} ì„ íƒìë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                
                if len(elements) > 5:
                    # ì œí•œ ì—†ì´ ëª¨ë“  ìš”ì†Œ ì²˜ë¦¬ (í˜ì´ì§€ ëê¹Œì§€)
                    for element in elements:
                        try:
                            title = element.get_text(strip=True)
                            if not title or len(title) < 5:
                                continue
                            
                            if title in seen_titles:
                                continue
                            
                            if self.should_exclude_post(title):
                                continue
                            
                            seen_titles.add(title)
                            
                            # URL êµ¬ì„±
                            href = element.get('href', '')
                            if href.startswith('/'):
                                post_url = self.base_url + href
                            elif href.startswith('http'):
                                post_url = href
                            else:
                                post_url = f"{self.base_url}/{href}"
                            
                            # ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸
                            if not any(pattern in href for pattern in ['best', 'document_srl', 'free']):
                                continue
                            
                            post_data = {
                                'title': title,
                                'url': post_url,
                                'site': self.site_name,
                                'category': 'ì¸ê¸°',
                                'author': 'ì—í¨ì½”ë¦¬ì•„',
                                'views': 0,
                                'likes': 1,
                                'comments': 0,
                                'popularity_score': 1
                            }
                            
                            post_list.append(post_data)
                            
                        except Exception as e:
                            continue
                    
                    if post_list:
                        break
            
            posts = post_list
            
            if not posts:
                print(f"âš ï¸  ì—í¨ì½”ë¦¬ì•„ ëª¨ë°”ì¼ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"âœ… ì—í¨ì½”ë¦¬ì•„ ëª¨ë°”ì¼ì—ì„œ {len(posts)}ê°œ ê²Œì‹œë¬¼ ì¶”ì¶œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì—í¨ì½”ë¦¬ì•„ ëª¨ë°”ì¼ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            self.cleanup()
        
        return posts
    
    def get_mobile_page_source_with_antibot(self, url: str) -> str:
        """ë´‡ íƒì§€ ìš°íšŒê°€ ê°•í™”ëœ í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        if not self.setup_mobile_browser():
            return None
        
        try:
            # ë´‡ íƒì§€ ìš°íšŒë¥¼ ìœ„í•œ ì¶”ê°€ í—¤ë” ì„¤ì •
            self.page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Upgrade-Insecure-Requests': '1'
            })
            
            print(f"ğŸŒ {self.site_name} í˜ì´ì§€ ì ‘ì† ì¤‘... (ë´‡ íƒì§€ ìš°íšŒ ëª¨ë“œ)")
            
            # ì²œì²œíˆ í˜ì´ì§€ ë¡œë“œ
            response = self.page.goto(url, wait_until='networkidle', timeout=30000)
            
            if response.status != 200:
                print(f"âŒ {self.site_name} HTTP ìƒíƒœ: {response.status}")
                return None
            
            # ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ê¸°
            print(f"ğŸ¤– {self.site_name} ì‚¬ëŒì²˜ëŸ¼ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜...")
            
            # ëœë¤ ë§ˆìš°ìŠ¤ ì›€ì§ì„
            for _ in range(5):
                x = random.randint(100, 300)
                y = random.randint(100, 400)
                self.page.mouse.move(x, y)
                time.sleep(random.uniform(0.5, 1.5))
            
            # ì²œì²œíˆ ìŠ¤í¬ë¡¤
            for i in range(5):
                self.page.evaluate(f"window.scrollTo(0, {i * 200})")
                time.sleep(random.uniform(2, 4))
            
            # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ (ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ)
            print(f"ğŸ“œ {self.site_name} í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
            
            for i in range(15):  # 15ë²ˆ ìŠ¤í¬ë¡¤
                self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(random.uniform(2, 4))  # 2~4ì´ˆ ëŒ€ê¸°
                
                # ë” ì´ìƒ ë¡œë“œí•  ì½˜í…ì¸ ê°€ ì—†ëŠ”ì§€ í™•ì¸
                current_height = self.page.evaluate("document.body.scrollHeight")
                time.sleep(1)
                new_height = self.page.evaluate("document.body.scrollHeight")
                
                if current_height == new_height and i > 5:
                    print(f"ğŸ“„ {self.site_name} ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ ì™„ë£Œ ({i+1}ë²ˆ ìŠ¤í¬ë¡¤)")
                    break
            
            # ë§¨ ìœ„ë¡œ ëŒì•„ê°€ê¸°
            self.page.evaluate("window.scrollTo(0, 0)")
            time.sleep(2)
            
            # ìŠ¤í¬ë¦°ìƒ· ì´¬ì˜
            screenshot_path = f"mobile_screenshot_{self.site_name}_{int(time.time())}.png"
            self.page.screenshot(path=screenshot_path, full_page=True)
            print(f"ğŸ“¸ {self.site_name} ëª¨ë°”ì¼ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.page.content()
            print(f"âœ… {self.site_name} í˜ì´ì§€ ì†ŒìŠ¤ íšë“ ({len(page_source)}ì)")
            
            return page_source
            
        except Exception as e:
            print(f"âŒ {self.site_name} ë´‡ íƒì§€ ìš°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None